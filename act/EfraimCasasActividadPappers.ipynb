{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "Presentación"
    ]
   },
   "source": [
    "<img src=\"../img/viu_logo.png\" width=\"200\">\n",
    "\n",
    "## 01MIAR - Actividad Whitepapers\n",
    "\n",
    "\n",
    "\n",
    "Autor: *EFRAIM ALBERTO CASAS*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artículo 01\n",
    "\n",
    "### The NumPy array: a structure for efficient numerical computation\n",
    "\n",
    "Van Der Walt, S., Colbert, S. C., &amp; Varoquaux, G. (2011). The NumPy array: A\n",
    "structure for efficient numerical computation. Computing in Science and\n",
    "Engineering, 13(2), 22-30. https://doi.org/10.1109/MCSE.2011.37\n",
    "\n",
    "https://www.researchgate.net/publication/224223550_The_NumPy_Array_A_Structure_for_Efficient_Numerical_Computation\n",
    "\n",
    "En este artículo se presenta la estructura *ndarray* de NumPy, y se hace un estudio sobre su uso y cómo mejora el rendimiento de ciertas operaciones matemáticas para la computación numérica.\n",
    "\n",
    "Se hace una breve introducción al *Broadcasting* como técnica que usa NumPy para realizar operaciones artiméticas sobre dos o más arrays con distintas dimensiones. \n",
    "\n",
    "- Actividad 01.01 - Ampliar dicha explicación, aportando posibles restricciones o limitaciones a dicho sistema y ejemplos propios de los casos de uso.\n",
    "\n",
    "También se introduce el trabajo con ficheros usando memoria mapeada.\n",
    "\n",
    "- Actividad 01.02 - Verificar la eficacia y mejora posible de rendimiento del uso de memoria mapeada sobre ndarrays de tamaños grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actividad 01\n",
    "\n",
    "Broadcasting es la propiedad de Numpy que permite realizar operaciones aritmeticas con arrays (matrices) de diferentes formas, esto junto con la propiedad de vectorización son las que brindan a Numpy gran parte de su versatilidad y potencia.  \n",
    "\n",
    "Cuando Numpy se encuentra con arrays que no tienen la misma forma y requiere hacer operaciones conjuntas sobre estos, aplica las siguentes regras\n",
    "\n",
    "1. Primera Regla\n",
    "\n",
    "Si dos arrays no tienen el mismo número de dimensiones, entonces se añadirá una dimensióm al principio del array con menor rango hasta que estos coincidan. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0 10]]]\n",
      "[5]\n",
      "[[[ 5 15]]]\n"
     ]
    }
   ],
   "source": [
    "#Ejemplo regla 1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "A = np.array([[ [ 0,10]]])\n",
    "\n",
    "B= np.array([5])\n",
    "\n",
    "print(A)\n",
    "print(B)\n",
    "\n",
    "print(A+B)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 2 12]]]\n"
     ]
    }
   ],
   "source": [
    "#otro ejemplo regla 1\n",
    "\n",
    "print (2+A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Segunda Regla\n",
    "\n",
    "SI un array tien un 1 en  alguna dimensión en particular actuará como si tuviera la longitud del array con mayor logitud en aquella dimensión. Esto implica que el valor en esta dimensión se repetirá hasta coincidir con la longitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[5]\n",
      "[5 6 7]\n"
     ]
    }
   ],
   "source": [
    "#Ejemplo Regla2\n",
    "\n",
    "A=np.arange(3) #Matriz 1x3\n",
    "print(A)\n",
    "\n",
    "B=np.array([5]) #Matriz 1x1\n",
    "print(B)\n",
    "\n",
    "print(A+B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "[0 1 2 3]\n",
      "[[1. 2. 3. 4.]\n",
      " [1. 2. 3. 4.]\n",
      " [1. 2. 3. 4.]\n",
      " [1. 2. 3. 4.]]\n"
     ]
    }
   ],
   "source": [
    "A=np.ones((4,4)) #Matriz 4x4\n",
    "print(A)\n",
    "B=np.arange(4) #Matriz 1x4\n",
    "print(B)\n",
    "\n",
    "print(A+B)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4  5  6  7  8  9 10]]\n",
      "[[ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]]\n",
      "[[ 2  3  4  5  6  7  8  9 10 11]\n",
      " [ 3  4  5  6  7  8  9 10 11 12]\n",
      " [ 4  5  6  7  8  9 10 11 12 13]\n",
      " [ 5  6  7  8  9 10 11 12 13 14]\n",
      " [ 6  7  8  9 10 11 12 13 14 15]\n",
      " [ 7  8  9 10 11 12 13 14 15 16]\n",
      " [ 8  9 10 11 12 13 14 15 16 17]\n",
      " [ 9 10 11 12 13 14 15 16 17 18]\n",
      " [10 11 12 13 14 15 16 17 18 19]\n",
      " [11 12 13 14 15 16 17 18 19 20]]\n"
     ]
    }
   ],
   "source": [
    "#Ejemplo regla 3\n",
    "A=np.arange(1,11).reshape(1,10)\n",
    "print(A)\n",
    "B=np.arange(1,11).reshape(10,1)\n",
    "print(B)\n",
    "print(A+B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Tercera Regla\n",
    "\n",
    "Si depues de aplicar las reglas 1 y 2 las dimensiones de los arrays siguen sin coincidir, entonces la operación no se puede realizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "[ 1 10]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,3) (2,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32024/1175022486.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,3) (2,) "
     ]
    }
   ],
   "source": [
    "A=np.arange(6).reshape(2,3) #Matriz 2x3\n",
    "B= B=np.array([1,10]) #Matriz 1x2\n",
    "\n",
    "print(A)\n",
    "print(B)\n",
    "print(A+B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 11]\n",
      " [ 3 13]\n",
      " [ 5 15]]\n"
     ]
    }
   ],
   "source": [
    "#la operación se podría dar si se hace un reshape de A\n",
    "\n",
    "print(A.reshape(3,2)+B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actividad 02\n",
    "\n",
    "Rendimiento del uso de memoria mapeada sobre ndarrays de tamaños grandes\n",
    "\n",
    "\n",
    "Ejemplo 1, \n",
    "Muesta como se hace el reshape de una matriz de 1'000.000 elementos. Para este caso Numpy conserva los mismos elementos en memoria y hace un cambio de la propiedad Strides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (1000000,)\n",
      "Estrides: (4,)\n",
      "Shape:  (1000, 1000)\n",
      "Estrides: (4000, 4)\n"
     ]
    }
   ],
   "source": [
    "A = np.arange(1000000)\n",
    "print(f\"Shape:  {A.shape}\")\n",
    "print(f\"Estrides: {A.strides}\")\n",
    "\n",
    "A.shape=(1000,1000)\n",
    "print(f\"Shape:  {A.shape}\")\n",
    "print(f\"Estrides: {A.strides}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver la operación solo tarda decimas de segundos, para redifinir la columna de un millón de elementos.\n",
    "\n",
    "\n",
    "Estrides: (4000, 4)\n",
    "\n",
    "Lo que Numpy hace internamente es modificar la propiedad strides: Cuando se hace el reshape, la propiedad strides cambia a (4000,4) esto quiere decir que para pasar a la siguiente file requiere desplazarse en memoria 4000 Bytes, mientras que para ir a la siguiente columna solo requiere desplazarse 4 Bytes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo cambiando la forma de la matriz a tres dimensiones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (10, 100, 1000)\n",
      "Estrides: (400000, 4000, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "A.shape=(10,100,1000)\n",
    "print(f\"Shape:  {A.shape}\")\n",
    "print(f\"Estrides: {A.strides}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusión:\n",
    "\n",
    "En cada uno de estos casos los array apuntan a la misma dirección en memoria, la diferencia de cada uno de los arreglos radica en la forma como se interpretan los datos basados en la propiedades, shape y strides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modificando el tipo de dato\n",
    "\n",
    "Tipo de datos\n",
    "En el anterior ejemplo veiamos que el deplazamiento entre cada columna estaba dado por 4 Bytes, esto sucede porque el tipo de dato es un enterod de 32 bits, para almacenar 32 bites se requieren 4 Bytes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo Dato: int32\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tipo Dato: {A.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo Dato: int16\n",
      "Estrides: (8000, 2)\n"
     ]
    }
   ],
   "source": [
    "#Cambiemos el tipo de dato\n",
    "B= np.empty((1000,1000))\n",
    "B.dtype=np.dtype('short')\n",
    "\n",
    "\n",
    "print(f\"Tipo Dato: {B.dtype}\")\n",
    "\n",
    "print(f\"Estrides: {B.strides}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este caso el desplazamiento por columnas solo es de 2 Bytes porque el tipo short corresponde a 16 bites, los cuales pueden ser representados por 2 Bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artículo 02\n",
    "\n",
    "### Data Structures for Statistical Computing in Python\n",
    "\n",
    "McKinney, W. (2010). Data Structures for Statistical Computing in Python.\n",
    "Proceedings of the 9th Python in Science Conference, December, 56-61.\n",
    "https://doi.org/10.25080/majora-92bf1922-00a\n",
    "\n",
    "https://www.researchgate.net/publication/265001241_Data_Structures_for_Statistical_Computing_in_Python\n",
    "\n",
    "En este segundo artículo el creador de *pandas* introduce dicha librería en comparación con las estructuras nativas de R.\n",
    "\n",
    "- Actividad 02.01 - Desarrollar una opinión razonada del estado actual de las herramientas de análisis de datos estadísticos en contraposición a como se muestran en el artículo, R vs Python vs SQL vs Others..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solución\n",
    "\n",
    "\n",
    "Desde el lanzamiento de Pandas y tal como vaticinaba Wes McKinney, autor del artículo y a su también creador de esta librería, esta se ha convertido no solo en una herramienta robusta para la computación estadística y el análisis de datos sino que casi en un estándar de la industria de ciencia de datos y machine learning.\n",
    "\n",
    "\n",
    "Las principales características que le han permitido a Pandas ser una de las herramientas preferidas a la hora de realizar análisis de datos, han sido, la facilidad de implementar funciones cotidianas para el tratamiento de datos:\n",
    "\n",
    "- Alinear Datos.\n",
    "- Depuración y operaciones con datos faltantes.\n",
    "- Combinación de Datasets (join).\n",
    "- Agrupamiento de Datos por variables categóricas.\n",
    "- Pivoteo de Datos.\n",
    "- Facilidad en implementación de modelos estadísticos. \n",
    "- Manejo de Fechas.\n",
    "\n",
    "Adicionalmente de la fácil interacción con otras librerías como son Matplotlib, SciPy e innumerables más que tienen como soporte para el manejo de datos a Pandas.\n",
    "\n",
    "No solo la facilidad y su sintaxis amigable han sido la clave del éxito de Pandas sino la velocidad de cómputo a la hora de procesar datos. Hoy en día lo vemos como algo de nuestro diario vivir, pero hace 10 años no era algo común. Esta doble capacidad que tiene Pandas ha permitido a la ciencia de Datos da un gran avance en términos de productividad. \n",
    "\n",
    "\n",
    "### Pandas vs SQL\n",
    "\n",
    "SQL (Structured Query Language) fue presupuesto en el año 1970 en el laboratorio de investigación de IBM en California y durante muchos años ha sido la fuente primaria para análisis de datos estructurados en Bases de Datos, tanto relacionales RDBMS y OLAP.\n",
    "\n",
    "Pandas aunque inicialmente inspirándose en R también ha tratado de solventar las capacidade de consulta del lenguaje SQL, aunque con paradigma diferente en varios aspectos.\n",
    "\n",
    "\n",
    "A continuación presentaré ejemplos de uso entre ambos lenguajes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Describir un set de datos o tabla*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "DESC empleados;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "empleados = pd.read_csv(\"/content/empleados.csv\")\n",
    "\n",
    "empleados.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Selección de columnas*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT CustomerId, Geography FROM CHURN;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empleados[['CustomerId', 'Geography']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Filtros* \n",
    "\n",
    "Selección de filas por algún o algunos criterios en particular "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT CustomerId, Geography FROM CHURN\n",
    "WHERE Geography = 'France'\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empleados[empleados.Geography == 'France'][['CustomerId','Geography']][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Valores únicos*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT DISTINCT Geography FROM CHURN;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empleados.Geography.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*cantidad de valores únicos*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT COUNT(DISTINCT Geography) FROM CHURN;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empleados.Geography.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Agrupacion de datos por categoría, funciones de agregación y ordenamiento*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT Geography, AVG(Age) AS 'Average Age' \n",
    "FROM CHURN\n",
    "GROUP BY Geography\n",
    "ORDER BY Age;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empleados[['Geography','Age']].groupby('Geography').mean()\\\n",
    ".sort_values(by='Age', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusión\n",
    "\n",
    "En la actualidad Pandas y SQL soportan muchas funcionalidades replicables mutuamente y en mi opinión personal no son herramientas excluyentes sino complementarias y las condiciones de cada caso de uso son las que determinan cual se acompla más a la solución  o también la posibilidad de utilizar ambas. \n",
    "\n",
    "Pandas como principal ventaja tien la posibilidad de uso de una gran cantidad de librerías las cuales facilitan el análisis de datos y donde ir uno o varios pasos más allá de describir la información es imperativo, ejemplo de estos son: modelos estadísticos, regresiones lineales, creación de modelo de machine learning entre muchos otros casos.\n",
    "\n",
    "SQL tiene como ventaja la opción de acceder a fuentes primarias de datos como bodegas de datos, creación de dashboards y funciones que utilizan datos en caliente. Las bases de Olap están optimizadas para almacenamiento y procesamiento de grandes cantidades de datos y los niveles de disponibilidad son altos.\n",
    "\n",
    "Muchas soluciones y en especial en ambientes corporativos pueden utilizar ambas tecnologías de forma complementaría y armónica, permitiendo a las rutinas creadas dentro de python cargar Pandas dataframes a partir de datos en tiempo real que estén en bases de datos y darle un procesamiento previo -si es necesario- mediante sentencias SQL para tener los datos mejor procesados, ejemplo join, filtros, cast entre un mar de posibilidades.\n",
    "\n",
    "Actualmente también he utilizado una solución que me ha parecido muy potente:  Mediantes sentencias SQL acceder a modelos de machine learning creados y entrenados en AWS Sage Maker con el apoyo de Pandas. Esta solución utilizando AWS redshift ML. Y permite poder validar modelos en tiempo real con toda la información disponible en una bodega de datos, sin tener que hacer cargas a otros sistemas para poder ser evaluada de forma muy inmediata.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a65927f5967699fcd722c82a9c185cfa0040c4b95e3af20a6eefdab7483f21b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
